{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e30711",
   "metadata": {},
   "source": [
    "# Q learning with NumPy, Mountain Car\n",
    "\n",
    "This notebook is implementing the basics of Q learning while creating the Q table from the ground up using NumPy. \n",
    "This notebook was built by following the instruction from the article wrttien by [Bradford Gill](https://medium.com/@bradfordgill/q-learning-with-numpy-mountain-car-feaef1c4ed1a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32da1b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "from celluloid import Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64384eff",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "The state space consists of two values, position and velocity. The allowed positions are in the between -1.2 and 0.6, with .6 being the goal. The velocity can exist between a range of -0.07 and 0.07. There are three actions accelerate in the negative direction, no velocity change, and accelerate in the positive direction. The staring position can be between -0.6 and 0.4. The velocity and state are updated by an equation seen in the Q_table class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd7edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITION_SPACE = (-1.2, .6)\n",
    "VELO_SPACE = (-.07, .07)\n",
    "ACTION_SPACE = [-1, 0, 1]\n",
    "START_RANGE = (-.6, -.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c2ee6",
   "metadata": {},
   "source": [
    "## Let build a Q Table class specific for this problem\n",
    "\n",
    "Since tables canâ€™t be infinite size, we must break the state space up into a discrete set of states. We will do this by hashing a state action pair to an index in the Q table, which is a NumPy ndarray. The Q table array will consist of position, velocity, and action dimensions whose lengths are specified as initialization parameters. The details of the class are in the comments, at a high level; the class will contain the Q table, a function to map states and actions to indexes, a function to get the next state given a state action pair, a function to update the Q value with the Bellman equation, and a few other utility functions that enable this to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab09c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_table():\n",
    "    def __init__(self, discrete_ps = 30, discrete_vs = 30, discrete_as = 3):\n",
    "        \"\"\"\n",
    "        discrete_ps: positon dim length \n",
    "        discrete_vs: velocity dim length \n",
    "        discrete_as: actions dim length \n",
    "        \"\"\"\n",
    "        self.p_len, self.v_len, self.a_len = discrete_ps, discrete_vs, discrete_as # store the size of each dimension\n",
    "        self.table = np.zeros((discrete_ps, discrete_vs, discrete_as)) # create the Q table initialized to 0\n",
    "        \n",
    "        # create mappings from position, velocity, and action to indexes\n",
    "        self.map_p_to_i = lambda p: floor((p - POSITION_SPACE[0])  / (POSITION_SPACE[1] - POSITION_SPACE[0]) * (discrete_ps - 1))\n",
    "        self.map_v_to_i = lambda v: floor((v - VELO_SPACE[0])  / (VELO_SPACE[1] - VELO_SPACE[0]) * (discrete_vs - 1))\n",
    "        self.map_a_to_i = lambda a: floor(a + 1)\n",
    "        mappings = [self.map_p_to_i, self.map_v_to_i, self.map_a_to_i]\n",
    "        self.map_pva_to_ijk = lambda p, v, a: [f(x) for x, f in zip([p, v, a], mappings)] # takes in p, v, and a returns the corresponding index \n",
    "        # this mapping may seem complicated but it limited my production time so I made the sacrifice \n",
    "\n",
    "    def get_value(self, p, v, a):\n",
    "        i, j, k = self.map_pva_to_ijk(p, v, a) # return corresponding index to p, v, a\n",
    "        return self.table[i, j, k]\n",
    "    \n",
    "    def get_random_action(self, *args): # args is included to allow p, v variables to be passed to function even though they are not need\n",
    "        return np.random.choice(ACTION_SPACE) # return a random action from the action space\n",
    "\n",
    "    def get_best_action(self, p, v): \n",
    "        return np.argmax(self.table[self.map_p_to_i(p), self.map_v_to_i(v), :]) - 1 # will return the best estimated action from [-1, 0, 1]\n",
    "\n",
    "    def update_state(self, p, v, a):\n",
    "        v += a  * 0.001 - np.cos(3 * p) * 0.0025 # update velocity \n",
    "        v = np.clip(v, *VELO_SPACE) # limit velocity to velocity space\n",
    "        p += v # increment p\n",
    "        if p < POSITION_SPACE[0] and v < 0: # if the agent is past the far left boundry and moving left\n",
    "            p = POSITION_SPACE[0] # set position to left edge of pos space\n",
    "            v = 0 # set velo = 0 \n",
    "        return p, v\n",
    "\n",
    "    def update_Q(self, reward, lr, discount, p, v, a): \n",
    "        \"\"\"\n",
    "        this updates the Q value based on the bellman equation: https://en.wikipedia.org/wiki/Bellman_equation\n",
    "        \"\"\"\n",
    "        i, j, k = self.map_pva_to_ijk(p, v, a)  # self.mappings countian the mapping of p, v, a to an index, the zip returns a\n",
    "        # tuple of a mapping function and a corresponding variable, \n",
    "        Q = self.table[i, j, k] \n",
    "        p_prime, v_prime = self.update_state(p, v, a) # tuple of new state\n",
    "        i_prime, j_prime = self.map_pva_to_ijk(p_prime, v_prime, 0)[:2] # get the indexes corresponding to the next state\n",
    "        i_prime, j_prime = min(self.p_len - 1, i_prime), min(self.v_len - 1, j_prime) # prevent the next state from being out of bounds\n",
    "        # bellman eq: Qnew = Q + lr(reward + discount * best_action_est, - Q) \n",
    "        self.table[i, j, k] = Q + lr * (reward + discount * np.max(self.table[i_prime, j_prime, :]) - Q) # update Q value \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990bcd80",
   "metadata": {},
   "source": [
    "# Let's enable the simulation\n",
    "The agent needs to explore the model and learn the value of actions given a state. \n",
    "There are multiple ways of doing this. One choice is to allow the model to search the environment until it reached the goal and iterate to the next episode as long as the simulation time was below a threshold. This often worked, but sometimes the agent would get stuck and run for millions of iterations. \n",
    "Alternatively, we can implement episodic tasks by only allowing 250 steps per episode, looping over a fixed number of episodes. The convergence on the second method was much faster.\n",
    "\n",
    "When it comes to learning, we need to balance the exploration and exploitation. We will use the epsilon greedy method, also refereed to as e-greedy. Epsilon represents the probability of exploring, hence a large epsilon will take a random action to explore the model and a low epsilon will take the greedy action with the highest estimated reward. Generally, it is best to have a high epsilon at the beginning of the simulation and lower it as you go through the simulation. I defined epsilon as 1 initially, giving the agent a 100% chance of exploring randomly. As the simulation continues, epsilon will be decayed by e_decay_rate until it reaches the minimum value of .01."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab690d",
   "metadata": {},
   "source": [
    "The simulation does the following: before the loop, create an array to store the reward from each function, initialize the Q table, define epsilon, and the epsilon decay function. The outer for loop goes over a fixed number of episodes. At the beginning of each episode, the position will be initialized to a value within the start range, the velocity will start at 0, and the reward will be set to 0. The inner while loop goes through the model step by step until 250 steps have been taken or the agent reaches the goal position. During each step the action is chosen at random or by maximum predicted reward, the Q table is updated given a reward of -1 since the agent has not reached the goal value, the next state is calculated from the action, and the cumulative reward counter is incremented. After the while loop ends, the cumulative reward of the episode is stored in the rewards array at the appropriate index and epsilon is decayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737b1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(discount, lr, e_decay_rate, num_episodes = 5000, return_table = False, \n",
    "             epsilon_min = 0.01, episode_length = 250):\n",
    "    rewards = np.zeros(num_episodes) # create an array of awards with an index for each episode\n",
    "    Qt = Q_table() # init Q table \n",
    "    epsilon = 1 # prob of exploring, 1 - epsilon = prob of exploiting (best)\n",
    "    e_decay = lambda e: max(e * e_decay_rate, epsilon_min) # how fast epsilon decays, the min value comes from deep minds paper\n",
    "    for i in range(num_episodes): # loop over each episode \n",
    "        p, v = np.random.uniform(*START_RANGE), 0 # init p and v for the episode \n",
    "        reward = 0 # set reward counter to 0 \n",
    "        while p < POSITION_SPACE[1] and reward < episode_length: # check if goal is reached and the episode isnt longer than episode_length \n",
    "            a_fn = np.random.choice([Qt.get_best_action, Qt.get_random_action], p = [1 - epsilon, epsilon]) # select exploiting function or exploring function \n",
    "            a = a_fn(p, v) # get approiate action\n",
    "            Qt.update_Q(-1, lr, discount, p, v, a) # update Q table\n",
    "            p, v = Qt.update_state(p, v, a) # get next state and action\n",
    "            reward += 1 # incremnent reward \n",
    "        rewards[i] = reward # store reward of this episode in the corresponding index\n",
    "        epsilon = e_decay(epsilon) # decay epsilon \n",
    "    if return_table:\n",
    "        return rewards, Qt # return the table if requested, allows me live graph the process \n",
    "    else:\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067e15a",
   "metadata": {},
   "source": [
    "First, I am going to do a random search for the optimal parameters. This process and take a while, so feel free to skip this step and take my results. If youâ€™re up to a challenge you can try to use [Bayesian Optimization](https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html) instead of random search, it has been shown to be more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da72fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_search():\n",
    "    \"\"\"\n",
    "    Lets do some hyperparam search! I am using an interative random search method. I will do a few trails, see what hyper params work and dont work then from\n",
    "    there iterate. \n",
    "    \"\"\"\n",
    "    \"\"\" outputs of the hyper param search \n",
    "    score: 211.72, discount: 0.2824933520659566, lr: 0.17244939592088193, epsilon_decay: 0.9422534370060514\n",
    "    score: 221.968, discount: 0.449881569041625, lr: 0.2797153971134617, epsilon_decay: 0.5612766255280709\n",
    "    score: 212.474, discount: 0.47365344896391615, lr: 0.34464527505672415, epsilon_decay: 0.7545763229312964\n",
    "    score: 147.08, discount: 0.8941097749008459, lr: 0.281720165576596, epsilon_decay: 0.6402003793646793 these are the best found hyper params \n",
    "    score: 191.864, discount: 0.694823346681323, lr: 0.1975957805539046, epsilon_decay: 0.6081500458326561\n",
    "    score: 223.624, discount: 0.5669016355520428, lr: 0.3532305508549993, epsilon_decay: 0.6683549632717782\n",
    "    score: 193.684, discount: 0.9403139125426704, lr: 0.38720828795289886, epsilon_decay: 0.9810833438406337\n",
    "    score: 175.858, discount: 0.4590723333145095, lr: 0.22499849644943165, epsilon_decay: 0.5712505297039662\n",
    "    score: 183.182, discount: 0.4603140408915277, lr: 0.1298694973714855, epsilon_decay: 0.7293091177345252\n",
    "    score: 250.0, discount: 0.15668291093964357, lr: 0.37399716877833344, epsilon_decay: 0.6667601971835406\n",
    "    score: 189.818, discount: 0.9302288566529271, lr: 0.3461452836964455, epsilon_decay: 0.5225729267470409\n",
    "    score: 206.89, discount: 0.7393388279997576, lr: 0.38404711840235606, epsilon_decay: 0.6978357947772437\n",
    "    score: 250.0, discount: 0.13798033479312335, lr: 0.3941747862632219, epsilon_decay: 0.9399145331098893\n",
    "    score: 198.196, discount: 0.7249406276042496, lr: 0.233962415206557, epsilon_decay: 0.65366939065216\n",
    "    score: 222.288, discount: 0.18603651792662607, lr: 0.10025206638754855, epsilon_decay: 0.746878295600906\n",
    "    score: 214.428, discount: 0.2933310640544758, lr: 0.11295598407531794, epsilon_decay: 0.9064368432918122\n",
    "    score: 179.178, discount: 0.6077255751840971, lr: 0.1592095997311603, epsilon_decay: 0.7834561753174173\n",
    "    score: 172.55, discount: 0.8588934763634241, lr: 0.11672460536562612, epsilon_decay: 0.700945100569841\n",
    "    score: 231.534, discount: 0.6922898626050366, lr: 0.25278751556240864, epsilon_decay: 0.8105051350601509\n",
    "    score: 248.862, discount: 0.1599106876760406, lr: 0.13941395688375366, epsilon_decay: 0.7350357615629758\n",
    "    \"\"\"\n",
    "    trials = 20 # number of trials to do in search \n",
    "    scores = [] # array to keep track of the score of each trial \n",
    "    discount_range = (.1, 1.) # range that discount can be between \n",
    "    lr_range = (.1, .4) # range that lr can be between\n",
    "    e_decay_range = (.5, 1.) # range that epsilon decay can be in \n",
    "    hp = {} # dict to store hyper params in \n",
    "    hp['discount'] = np.random.uniform(*discount_range, trials)\n",
    "    hp['lr'] = np.random.uniform(*lr_range, trials)\n",
    "    hp['epsilon_decay'] = np.random.uniform(*e_decay_range, trials)\n",
    "\n",
    "    for i in range(trials): # for each trail\n",
    "        hold = (hp['discount'][i], hp['lr'][i], hp['epsilon_decay'][i]) # tuple hold for func params\n",
    "        score = simulate(*hold) # simulate with given hyper params\n",
    "        scores.append(score) # append score to scores\n",
    "        print(\"score: {}, discount: {}, lr: {}, epsilon_decay: {}\".format(np.average(score[-500:]), *hold)) # print results \n",
    "\n",
    "    fig, axs = plt.subplots(3, 1) # create figure and 3 axes rows \n",
    "    for ax, key in zip(axs, hp.keys()): \n",
    "        ax.scatter(hp[key], np.average(scores[i])) # plot values for visual analysis \n",
    "        ax.set_title(key) # title graph \n",
    "    fig.show()\n",
    "\n",
    "# hyper_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86d606a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter Pillow unavailable; using Pillow instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def train_and_plot(lr, discount, epsilon_decay): # train the graph and show the \n",
    "    _, Qt = simulate(lr, discount, epsilon_decay, return_table = True) # get a Q table given hyper params \n",
    "    N = 400  \n",
    "    x = np.linspace(*POSITION_SPACE, N) # linearly spaced array across the positon space with N indexes \n",
    "    fn = lambda x: np.sin(3*x) # function that turns x position to y value (makes the mountians)\n",
    "    y = fn(x) # create y array \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (8, 5)) # create a figure and axes\n",
    "    camera = Camera(fig) # create the Camera for the animation \n",
    "    p, v = -.5, 0. # init and v \n",
    "    while p < POSITION_SPACE[1]: # run sim until \n",
    "        a = Qt.get_best_action(p, v) # get the best action given the state\n",
    "        p, v = Qt.update_state(p, v, a) # update the state based on the action \n",
    "        ax.plot(x, y, c = 'b', zorder=0) # plot mounitans repeative but im not memory constrained so im not going to rewrite\n",
    "        ax.scatter(p, fn(p), s = 25, c = 'r', zorder=5) # plot where the mountian car is, having a z order larger than the line specifies this will be on top\n",
    "        ax.set_axis_off()\n",
    "        camera.snap() # snap the frame \n",
    "    animation = camera.animate()\n",
    "    HTML(animation.to_html5_video())\n",
    "    animation.save('mountiancar.gif', writer='Pillow') # save the animation as a gif (located in this folder)\n",
    "    \n",
    "if __name__ == \"__main__\": # \n",
    "    train_and_plot(.894, .282, .640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48c006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-tutorial",
   "language": "python",
   "name": "rl-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
